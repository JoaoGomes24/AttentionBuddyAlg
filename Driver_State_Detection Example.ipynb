{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd0f5f50226d34cd2a04a2575e18851b6ddf357e29320c8cf0af3b8baeba2eb381a",
   "display_name": "Python 3.9.2 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "f5f50226d34cd2a04a2575e18851b6ddf357e29320c8cf0af3b8baeba2eb381a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import dlib\n",
    "import math, time\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "source": [
    "### Camera Parameters (Optional)\n",
    "Camera parameters (camera matrix + distorsion coefficients) obtained from the script and chessboard photos inside the folder \"Camera_Calibration\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_matrix = np.array(\n",
    "    [[1.10481797e+03, 0.00000000e+00, 9.76862669e+02],\n",
    "     [0.00000000e+00, 1.11775382e+03, 5.07678687e+02],\n",
    "     [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]], dtype=\"double\")\n",
    "# camera matrix obtained from the camera calibration script, using a 9x6 chessboard\n",
    "\n",
    "dist_coeffs = np.array(\n",
    "    [[0.13580439, -0.08770944, -0.01913275, -0.00170941, -0.0361747]], dtype=\"double\")\n",
    "\n",
    "# distortion coefficients obtained from the camera calibration script, using a 9x6 chessboard"
   ]
  },
  {
   "source": [
    "### Utility Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(frame, scale_percent):\n",
    "    \"\"\"\n",
    "    Resize the image maintaining the aspect ratio\n",
    "    :param frame: opencv image/frame\n",
    "    :param scale_percent: int\n",
    "        scale factor for resizing the image\n",
    "    :return:\n",
    "    resized: rescaled opencv image/frame\n",
    "    \"\"\"\n",
    "    width = int(frame.shape[1] * scale_percent / 100)\n",
    "    height = int(frame.shape[0] * scale_percent / 100)\n",
    "    dim = (width, height)\n",
    "\n",
    "    resized = cv2.resize(frame, dim, interpolation=cv2.INTER_LINEAR)\n",
    "    return resized\n",
    "\n",
    "\n",
    "def get_face_area(face):\n",
    "    \"\"\"\n",
    "    :param face: dlib bounding box of a detected face in faces\n",
    "    :return: area of the face bounding box\n",
    "    \"\"\"\n",
    "    return (face.left() - face.right()) * (face.bottom() - face.top())\n",
    "\n",
    "\n",
    "def show_keypoints(keypoints, frame):\n",
    "    \"\"\"\n",
    "\n",
    "    :param keypoints: dlib iterable 68 keypoints object\n",
    "    :param frame: opencv frame\n",
    "    :return: frame\n",
    "        Returns the frame with all the 68 dlib face keypoints drawn\n",
    "    \"\"\"\n",
    "    for n in range(0, 68):  # per tutti i 68 keypoints stampa su frame la loro posizione\n",
    "        x = keypoints.part(n).x\n",
    "        y = keypoints.part(n).y\n",
    "        cv2.circle(frame, (x, y), 1, (0, 0, 255), -1)\n",
    "        return frame\n",
    "\n",
    "\n",
    "def midpoint(p1, p2):\n",
    "    \"\"\"\n",
    "    Compute the midpoint between two dlib keypoints\n",
    "\n",
    "    :param p1: dlib single keypoint\n",
    "    :param p2: dlib single keypoint\n",
    "    :return: array of x,y coordinated of the midpoint between p1 and p2\n",
    "    \"\"\"\n",
    "    return np.array([int((p1.x + p2.x) / 2), int((p1.y + p2.y) / 2)])\n",
    "\n",
    "\n",
    "def get_array_keypoints(landmarks, dtype=\"int\", verbose: bool = False):\n",
    "    \"\"\"\n",
    "    Converts all the iterable dlib 68 face keypoint in a numpy array of shape 68,2\n",
    "\n",
    "    :param landmarks: dlib iterable 68 keypoints object\n",
    "    :param dtype: dtype desired in output\n",
    "    :param verbose: if set to True, prints array of keypoints (default is False)\n",
    "    :return: points_array\n",
    "        Numpy array containing all the 68 keypoints (x,y) coordinates\n",
    "        The shape is 68,2\n",
    "    \"\"\"\n",
    "    points_array = np.zeros((68, 2), dtype=dtype)\n",
    "    for i in range(0, 68):\n",
    "        points_array[i] = (landmarks.part(i).x, landmarks.part(i).y)\n",
    "\n",
    "    if verbose:\n",
    "        print(points_array)\n",
    "\n",
    "    return points_array\n",
    "\n",
    "\n",
    "def isRotationMatrix(R):\n",
    "    \"\"\"\n",
    "\n",
    "    :param R: np.array matrix of 3 by 3\n",
    "    :return: True or False\n",
    "        Return True if a matrix is a rotation matrix, False if not\n",
    "    \"\"\"\n",
    "    Rt = np.transpose(R)\n",
    "    shouldBeIdentity = np.dot(Rt, R)\n",
    "    I = np.identity(3, dtype=R.dtype)\n",
    "    n = np.linalg.norm(I - shouldBeIdentity)\n",
    "    return n < 1e-6\n",
    "\n",
    "\n",
    "def rotationMatrixToEulerAngles(R):\n",
    "    \"\"\"\n",
    "    Computes the Tait–Bryan Euler angles from a Rotation Matrix.\n",
    "    Also checks if there is a gymbal lock and eventually use an alternative formula\n",
    "    :param R: np.array\n",
    "        3 x 3 Rotation matrix\n",
    "    :return: (roll, pitch, yaw) tuple of float numbers\n",
    "        Euler angles in radians\n",
    "    \"\"\"\n",
    "    # Calculates Tait–Bryan Euler angles from a Rotation Matrix\n",
    "    assert (isRotationMatrix(R))  # check if it's a Rmat\n",
    "\n",
    "    sy = math.sqrt(R[0, 0] * R[0, 0] + R[1, 0] * R[1, 0])\n",
    "    singular = sy < 1e-6\n",
    "\n",
    "    if not singular:  # check if it's a gymbal lock situation\n",
    "        x = math.atan2(R[2, 1], R[2, 2])\n",
    "        y = math.atan2(-R[2, 0], sy)\n",
    "        z = math.atan2(R[1, 0], R[0, 0])\n",
    "\n",
    "    else:  # if in gymbal lock, use different formula for yaw, pitch roll\n",
    "        x = math.atan2(-R[1, 2], R[1, 1])\n",
    "        y = math.atan2(-R[2, 0], sy)\n",
    "        z = 0\n",
    "\n",
    "    return np.array([x, y, z])\n",
    "\n",
    "\n",
    "def draw_pose_info(frame, img_point, point_proj, roll=None, pitch=None, yaw=None):\n",
    "    \"\"\"\n",
    "    Draw 3d orthogonal axis given a frame, a point in the frame, the projection point array.\n",
    "    Also prints the information about the roll, pitch and yaw if passed\n",
    "\n",
    "    :param frame: opencv image/frame\n",
    "    :param img_point: tuple\n",
    "        x,y position in the image/frame for the 3d axis for the projection\n",
    "    :param point_proj: np.array\n",
    "        Projected point along 3 axis obtained from the cv2.projectPoints function\n",
    "    :param roll: float, optional\n",
    "    :param pitch: float, optional\n",
    "    :param yaw: float, optional\n",
    "    :return: frame: opencv image/frame\n",
    "        Frame with 3d axis drawn and, optionally, the roll,pitch and yaw values drawn\n",
    "    \"\"\"\n",
    "    frame = cv2.line(frame, img_point, tuple(\n",
    "        point_proj[0].ravel().astype(int)), (255, 0, 0), 3)\n",
    "    frame = cv2.line(frame, img_point, tuple(\n",
    "        point_proj[1].ravel().astype(int)), (0, 255, 0), 3)\n",
    "    frame = cv2.line(frame, img_point, tuple(\n",
    "        point_proj[2].ravel().astype(int)), (0, 0, 255), 3)\n",
    "\n",
    "    if roll is not None and pitch is not None and yaw is not None:\n",
    "        cv2.putText(frame, \"Roll:\" + str(round(roll, 3)), (400, 50),\n",
    "                    cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "        cv2.putText(frame, \"Pitch:\" + str(round(pitch, 3)), (400, 70),\n",
    "                    cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "        cv2.putText(frame, \"Yaw:\" + str(round(yaw, 3)), (400, 90),\n",
    "                    cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "    return frame\n"
   ]
  },
  {
   "source": [
    "### Eye Detector Class\n",
    "This class include methods for computing EAR score (Eye Aspect Ratio), eye ROI for each eyes and Gaze Score (how much the pupil is offsetted in respect to the eye center, for each eyes)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Eye_Detector:\n",
    "\n",
    "    def __init__(self, frame, landmarks, show_processing: bool = False):\n",
    "        \"\"\"\n",
    "        Eye dector object that contains various method for eye aperture rate estimation and gaze score estimation\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        frame: opencv/numpy image array\n",
    "            contains frame to be processed\n",
    "        landmarks:\n",
    "            list of landmarks detected with dlib face 68 keypoint detector \n",
    "        show_processing: bool\n",
    "            If set to True, shows frame images during the processing in some steps (default is False)\n",
    "\n",
    "        Methods\n",
    "        ----------\n",
    "        - show_eye_keypoints: shows eye keypoints in the frame/image\n",
    "        - get_EAR: computes EAR average score for the two eyes of the face\n",
    "        - get_ROI: finds the ROI (Region Of Interest) of the eye, given the keypoints\n",
    "        - get_Gaze_Score: computes the Gaze_Score (normalized euclidean distance between center of eye and pupil)\n",
    "            of the eyes of the face\n",
    "        \"\"\"\n",
    "        self.keypoints = landmarks\n",
    "        self.frame = frame\n",
    "        self.show_processing = show_processing\n",
    "\n",
    "    def show_eye_keypoints(self, color_frame):\n",
    "        \"\"\"\n",
    "        Shows eyes keypoints found in the face, drawing red circles in their position in the frame/image\n",
    "        :param color_frame: opencv frame/image\n",
    "        \"\"\"\n",
    "\n",
    "        for n in range(36, 48):\n",
    "            x = self.keypoints.part(n).x\n",
    "            y = self.keypoints.part(n).y\n",
    "            cv2.circle(color_frame, (x, y), 1, (0, 0, 255), -1)\n",
    "        return\n",
    "\n",
    "    def get_EAR(self):\n",
    "        \"\"\"\n",
    "\n",
    "        :return: ear_score\n",
    "            EAR average score between the two eyes\n",
    "            The EAR or Eye Aspect Ratio is computed as the eye opennes divided by the eye lenght\n",
    "            Each eye has his scores and the two scores are averaged\n",
    "        \"\"\"\n",
    "        pts = self.keypoints\n",
    "        i = 0  # auxiliary counter\n",
    "        eye_pts_l = np.zeros(shape=(6, 2))  # numpy array for storing the keypoints positions of the left eye\n",
    "        eye_pts_r = np.zeros(shape=(6, 2))  # numpy array for storing the keypoints positions of the right eye\n",
    "\n",
    "        for n in range(36, 42):  # the dlib keypoints from 36 to 42 are referring to the left eye\n",
    "            point_l = pts.part(n)  # save the i-keypoint of the left eye\n",
    "            point_r = pts.part(n + 6)  # save the i-keypoint of the right eye\n",
    "            eye_pts_l[i] = [point_l.x, point_l.y]  # array of x,y coordinates for the left eye reference point\n",
    "            eye_pts_r[i] = [point_r.x, point_r.y]  # array of x,y coordinates for the right eye reference point\n",
    "            i += 1  # increasing the auxiliary counter\n",
    "\n",
    "        def EAR_eye(eye_pts):\n",
    "            \"\"\"\n",
    "            Computer the EAR score for a single eyes given it's keypoints\n",
    "            :param eye_pts: numpy array of shape (6,2) containing the keypoints of an eye considering the dlib ordering\n",
    "            :return: ear_eye\n",
    "                EAR of the eye\n",
    "            \"\"\"\n",
    "            ear_eye = (LA.norm(eye_pts[1] - eye_pts[5]) + LA.norm(\n",
    "                eye_pts[2] - eye_pts[4])) / (2 * LA.norm(eye_pts[0] - eye_pts[3]))\n",
    "            '''\n",
    "            EAR is computed as the mean of two measures of eye opening (see dlib face keypoints for the eye)\n",
    "            divided by the eye lenght\n",
    "            '''\n",
    "            return ear_eye\n",
    "\n",
    "        ear_left = EAR_eye(eye_pts_l)  # computing the left eye EAR score\n",
    "        ear_right = EAR_eye(eye_pts_r)  # computing the right eye EAR score\n",
    "\n",
    "        # computing the average EAR score\n",
    "        ear_avg = (ear_left + ear_right) / 2\n",
    "\n",
    "        return ear_avg\n",
    "\n",
    "    def get_ROI(self, left_corner_keypoint_num: int):\n",
    "        \"\"\"\n",
    "        Get the ROI bounding box of the eye given one of it's dlib keypoint found in the face\n",
    "\n",
    "        :param left_corner_keypoint_num: most left dlib keypoint of the eye\n",
    "        :return: eye_roi\n",
    "            Sub-frame of the eye region of the opencv frame/image\n",
    "        \"\"\"\n",
    "\n",
    "        keypoints = self.keypoints\n",
    "        kp_num = left_corner_keypoint_num\n",
    "\n",
    "        left_point = np.array(\n",
    "            [keypoints.part(kp_num).x, keypoints.part(kp_num).y])  # left point of the eye\n",
    "        right_point = np.array(\n",
    "            [keypoints.part(kp_num + 3).x, keypoints.part(kp_num + 3).y])  # right point of the eye\n",
    "        upper_point = midpoint(keypoints.part(kp_num + 1),\n",
    "                               keypoints.part(kp_num + 1))  # upper-mid point of the eye\n",
    "        lower_point = midpoint(keypoints.part(kp_num + 4),\n",
    "                               keypoints.part(kp_num + 5))  # lower-mid point od the eye\n",
    "\n",
    "        eye_width = LA.norm(left_point - right_point)  # eye-width (l2 norm)\n",
    "\n",
    "        '''\n",
    "        Additional parameters that can be useful:\n",
    "        \n",
    "        eye_height = LA.norm(upper_point - lower_point)  # eye-height (l2 norm)\n",
    "        roi_upleft = ( (left_point[0] +1) , (left_point[1] - int(eye_width/2) +1) )\n",
    "        roi_btmright = ( (right_point[0] +1), (right_point[1] + int(eye_width/2) +1) )\n",
    "        '''\n",
    "\n",
    "        eye_roi = self.frame[(left_point[1] - int(eye_width / 2)):(right_point[1] + int(eye_width / 2)),\n",
    "                  left_point[0]:right_point[0]]  # sub-portion of the image/frame containing the eye\n",
    "\n",
    "        return eye_roi\n",
    "\n",
    "    def get_Gaze_Score(self):\n",
    "        \"\"\"\n",
    "        Computes the average Gaze Score for the eyes\n",
    "        The Gaze Score is the mean of the l2 norm (euclidean distance) between the center point of the Eye ROI\n",
    "        (eye bounding box) and the center of the eye-pupil\n",
    "\n",
    "        :return: avg_gaze_score or None\n",
    "            If successful, returns the float gaze score\n",
    "            If unsuccessful, returns None\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        def get_gaze(eye_roi):\n",
    "            \"\"\"\n",
    "            Computes the L2 norm between the center point of the Eye ROI\n",
    "            (eye bounding box) and the center of the eye pupil\n",
    "            :param eye_roi: float\n",
    "            :return: (gaze_score, eye_roi): tuple\n",
    "                tuple\n",
    "            \"\"\"\n",
    "\n",
    "            eye_center = np.array(\n",
    "                [(eye_roi.shape[1] // 2), (eye_roi.shape[0] // 2)])\n",
    "            gaze_score = None\n",
    "            circles = None\n",
    "            contours = None\n",
    "\n",
    "            eye_roi = cv2.bilateralFilter(eye_roi, 3, 25, 25)\n",
    "            eye_tresh = cv2.adaptiveThreshold(\n",
    "                eye_roi, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 15, 7)\n",
    "\n",
    "            contours, _ = cv2.findContours(\n",
    "                eye_tresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "            if contours is not None and len(contours) > 0:\n",
    "                contours = sorted(\n",
    "                    contours, key=lambda x: cv2.contourArea(x), reverse=True)\n",
    "                cnt = contours[0]\n",
    "                cv2.drawContours(eye_roi, [cnt], -1, (255, 255, 255), 1)\n",
    "\n",
    "                circles = cv2.HoughCircles(eye_roi, cv2.HOUGH_GRADIENT, 1, 10,\n",
    "                                           param1=200, param2=6, minRadius=1, maxRadius=5)\n",
    "\n",
    "                if circles is not None and len(circles) > 0:\n",
    "                    circles = np.uint16(np.around(circles))\n",
    "                    circle = circles[0][0, :]\n",
    "                    # cv2.circle(eye_roi,(i[0],i[1]),i[2],(255,255,255),1)\n",
    "                    cv2.circle(\n",
    "                        eye_roi, (circle[0], circle[1]), 1, (255, 255, 255), -1)\n",
    "                    pupil_position = np.array([int(circle[0]), int(circle[1])])\n",
    "                    cv2.line(eye_roi, (eye_center[0], eye_center[1]), (\n",
    "                        pupil_position[0], pupil_position[1]), (255, 255, 255), 1)\n",
    "                    gaze_score = LA.norm(\n",
    "                        pupil_position - eye_center) / eye_center[0]\n",
    "\n",
    "            cv2.circle(eye_roi, (eye_center[0],\n",
    "                                 eye_center[1]), 1, (0, 0, 0), -1)\n",
    "\n",
    "            if gaze_score is not None:\n",
    "                return gaze_score, eye_roi\n",
    "            else:\n",
    "                return None, None\n",
    "\n",
    "        left_eye_ROI = self.get_ROI(36)\n",
    "        right_eye_ROI = self.get_ROI(42)\n",
    "\n",
    "        gaze_eye_left, left_eye = get_gaze(left_eye_ROI)\n",
    "        gaze_eye_right, right_eye = get_gaze(right_eye_ROI)\n",
    "\n",
    "        if self.show_processing and (left_eye is not None) and (right_eye is not None):\n",
    "            left_eye = resize(left_eye, 1000)\n",
    "            right_eye = resize(right_eye, 1000)\n",
    "            cv2.imshow(\"left eye\", left_eye)\n",
    "            cv2.imshow(\"right eye\", right_eye)\n",
    "\n",
    "        if gaze_eye_left and gaze_eye_right:\n",
    "\n",
    "            avg_gaze_score = (gaze_eye_left + gaze_eye_left) / 2\n",
    "            return avg_gaze_score\n",
    "\n",
    "        else:\n",
    "            return None\n"
   ]
  },
  {
   "source": [
    "### Head Pose Estimator Class\n",
    "This class contains the method used to compute the roll, pitch and yaw of the head, goven the frame captured and some of the dlib face keypoints"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Head_Pose_Estimator:\n",
    "\n",
    "    def __init__(self, frame, landmarks, camera_matrix=None, dist_coeffs=None, verbose: bool = False):\n",
    "        \"\"\"\n",
    "        Head Pose estimator object that contains the get_pose method for computing the three euler angles\n",
    "        (roll, pitch, yaw) of the head. It uses the image/frame, the dlib detected landmarks of the head and,\n",
    "        optionally the camera parameters\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        frame: opencv image array\n",
    "            contains frame to be processed\n",
    "        landmarks:\n",
    "            list of landmarks detected with dlib face 68 keypoint detector\n",
    "        verbose: bool\n",
    "            If set to True, shows the head pose axis projected from the nose keypoint and the face landmarks points\n",
    "            used for pose estimation (default is False)\n",
    "        \"\"\"\n",
    "\n",
    "        self.verbose = verbose\n",
    "        self.keypoints = landmarks  # dlib 68 landmarks\n",
    "        self.frame = frame  # opencv image array\n",
    "\n",
    "        self.axis = np.float32([[200, 0, 0],\n",
    "                                [0, 200, 0],\n",
    "                                [0, 0, 200]])\n",
    "        # array that specify the length of the 3 projected axis from the nose\n",
    "\n",
    "        if camera_matrix is None:\n",
    "            # if no camera matrix is given, estimate camera parameters using picture size\n",
    "            self.size = frame.shape\n",
    "            self.focal_length = self.size[1]\n",
    "            self.center = (self.size[1] / 2, self.size[0] / 2)\n",
    "            self.camera_matrix = np.array(\n",
    "                [[self.focal_length, 0, self.center[0]],\n",
    "                 [0, self.focal_length, self.center[1]],\n",
    "                 [0, 0, 1]], dtype=\"double\"\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            # take camera matrix\n",
    "            self.camera_matrix = camera_matrix\n",
    "\n",
    "        if dist_coeffs is None:  # if no distorsion coefficients are given, assume no lens distortion\n",
    "            self.dist_coeffs = np.zeros((4, 1))\n",
    "        else:\n",
    "            # take camera distortion coefficients\n",
    "            self.dist_coeffs = dist_coeffs\n",
    "\n",
    "        # 3D Head model world space points (generic human head)\n",
    "        self.model_points = np.array([\n",
    "            (0.0, 0.0, 0.0),  # Nose tip\n",
    "            (0.0, -330.0, -65.0),  # Chin\n",
    "            (-225.0, 170.0, -135.0),  # Left eye left corner\n",
    "            (225.0, 170.0, -135.0),  # Right eye right corner\n",
    "            (-150.0, -150.0, -125.0),  # Left Mouth corner\n",
    "            (150.0, -150.0, -125.0)  # Right mouth corner\n",
    "\n",
    "        ])\n",
    "        # 2D Point position of dlib face keypoints used for pose estimation\n",
    "        self.image_points = np.array([\n",
    "            (landmarks.part(30).x, landmarks.part(30).y),  # Nose tip\n",
    "            (landmarks.part(8).x, landmarks.part(8).y),  # Chin\n",
    "            (landmarks.part(36).x, landmarks.part(\n",
    "                36).y),  # Left eye left corner\n",
    "            (landmarks.part(45).x, landmarks.part(\n",
    "                45).y),  # Right eye right corne\n",
    "            (landmarks.part(48).x, landmarks.part(\n",
    "                48).y),  # Left Mouth corner\n",
    "            (landmarks.part(54).x, landmarks.part(\n",
    "                54).y)  # Right mouth corner\n",
    "        ], dtype=\"double\")\n",
    "\n",
    "    def get_pose(self):\n",
    "        \"\"\"\n",
    "        Estimate head pose using the head pose estimator object instantiated attribute\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        - if successful: image_frame, roll, pitch, yaw (tuple)\n",
    "        - if unsuccessful: None,None,None,None (tuple)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        (success, rvec, tvec) = cv2.solvePnP(self.model_points, self.image_points,\n",
    "                                             self.camera_matrix, self.dist_coeffs, flags=cv2.SOLVEPNP_ITERATIVE)\n",
    "        '''\n",
    "        The OpenCV Solve PnP method computes the rotation and translation vectors with respect to the camera coordinate \n",
    "        system of the image_points referred to the 3d head model_points. It takes into account the camera matrix and\n",
    "        the distortion coefficients.\n",
    "        The method used is iterative (cv2.SOLVEPNP_ITERATIVE)\n",
    "        An alternative method can be the cv2.SOLVEPNP_SQPNP\n",
    "        '''\n",
    "\n",
    "        if success:  # if the solvePnP succeed, compute the head pose, otherwise return None\n",
    "\n",
    "            rvec, tvec = cv2.solvePnPRefineVVS(\n",
    "                self.model_points, self.image_points, self.camera_matrix, self.dist_coeffs, rvec, tvec)\n",
    "            # this method is used to refine the rvec and tvec prediction\n",
    "\n",
    "            nose = (int(self.image_points[0][0]), int(self.image_points[0][1]))  # Head nose point in the image plane\n",
    "\n",
    "            (nose_end_point2D, jacobian) = cv2.projectPoints(\n",
    "                self.axis, rvec, tvec, self.camera_matrix, self.dist_coeffs)\n",
    "            # this function computes the 3 projection axis from the nose point of the head, so we can use them to\n",
    "            # show the head pose later\n",
    "\n",
    "            Rmat = cv2.Rodrigues(rvec)[0]\n",
    "            # using the Rodrigues formula, this functions computes the Rotation Matrix from the rotation vector\n",
    "\n",
    "            roll, pitch, yaw = 180 * (rotationMatrixToEulerAngles(Rmat) / np.pi)\n",
    "            \"\"\"\n",
    "            We use the rotationMatrixToEulerAngles function to compute the euler angles (roll, pitch, yaw) from the\n",
    "            Rotation Matrix. This function also checks if we have a gymbal lock.\n",
    "            The angles are converted from radians to degrees \n",
    "            \"\"\"\n",
    "\n",
    "            \"\"\"\n",
    "            An alternative method to compute the euler angles is the following:\n",
    "            \n",
    "            P = np.hstack((Rmat,tvec)) -> computing the projection matrix\n",
    "            euler_angles = -cv2.decomposeProjectionMatrix(P)[6] -> extracting euler angles for yaw pitch and roll from the projection matrix\n",
    "            \"\"\"\n",
    "\n",
    "            if self.verbose:\n",
    "                # print(\"Camera Matrix :\\n {0}\".format(self.camera_matrix))\n",
    "                # print (\"Rotation Vector:\\n {0}\".format(rvec))\n",
    "                # print (\"Translation Vector:\\n {0}\".format(tvec))\n",
    "                # print(\"Roll:\"+ str(roll) + \" Pitch: \" + str(pitch) + \" Yaw: \" + str(yaw))\n",
    "                self.frame = draw_pose_info(\n",
    "                    self.frame, nose, nose_end_point2D, roll, pitch, yaw)\n",
    "                # draws 3d axis from the nose and to the computed projection points\n",
    "                for point in self.image_points:\n",
    "                    cv2.circle(self.frame, tuple(\n",
    "                        point.ravel().astype(int)), 2, (0, 255, 255), -1)\n",
    "                # draws the 6 keypoints used for the pose estimation\n",
    "\n",
    "            return self.frame, roll, pitch, yaw\n",
    "\n",
    "        else:\n",
    "            return None, None, None, None\n"
   ]
  },
  {
   "source": [
    "### Attention Scorer Class\n",
    "This class takes into account the framerate of the webcam stream and the various values and tresholds for EAR, Gaze Score and Head Pose. Then, given a time limit for each score when the threshold is surpassed, it returns the state of the driver.\n",
    "The returned values are three:\n",
    "\n",
    "1. Asleep\n",
    "2. Looking away\n",
    "3. Distracted"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Attention_Scorer:\n",
    "\n",
    "    def __init__(self, capture_fps: int, ear_tresh, gaze_tresh, perclos_tresh=0.2, ear_time_tresh=4.0, pitch_tresh=35,\n",
    "                 yaw_tresh=30, gaze_time_tresh=4.0, roll_tresh=None, pose_time_tresh=4.0, verbose=False):\n",
    "\n",
    "        \"\"\"\n",
    "        Attention Scorer class that contains methods for estimating EAR,Gaze_Score,PERCLOS and Head Pose over time,\n",
    "        with the given thresholds (time tresholds and value tresholds)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        capture_fps: int\n",
    "            Upper frame rate of video/capture stream considered\n",
    "\n",
    "        ear_tresh: float or int\n",
    "            EAR score value threshold (if the EAR score is less than this value, eyes are considered closed!)\n",
    "\n",
    "        gaze_tresh: float or int\n",
    "            Gaze Score value treshold (if the Gaze Score is more than this value, the gaze is considered not centered)\n",
    "\n",
    "        perclos_tresh: float (ranges from 0 to 1)\n",
    "            PERCLOS treshold that indicates the maximum time allowed in 60 seconds of eye closure\n",
    "            (default is 0.2 -> 20% of 1 minute)\n",
    "\n",
    "        ear_time_tresh: float or int\n",
    "            Maximum time allowable for consecutive eye closure (given the EAR threshold considered)\n",
    "            (default is 4.0 seconds)\n",
    "\n",
    "        pitch_tresh: int\n",
    "            Treshold of the pitch angle for considering the person distracted (not looking in front)\n",
    "            (default is 35 degrees from the center position)\n",
    "\n",
    "        yaw_tresh: int\n",
    "            Treshold of the yaw angle for considering the person distracted/unconscious (not straight neck)\n",
    "            (default is 30 degrees from the straight neck position)\n",
    "\n",
    "        roll_tresh: int\n",
    "            Treshold of the roll angle for considering the person distracted/unconscious (not straight neck)\n",
    "            (default is None: not considered)\n",
    "\n",
    "        pose_time_tresh: float or int\n",
    "            Maximum time allowable for consecutive distracted head pose (given the pitch,yaw and roll thresholds)\n",
    "            (default is 4.0 seconds)\n",
    "\n",
    "        verbose: bool\n",
    "            If set to True, print additional information about the scores (default is False)\n",
    "\n",
    "\n",
    "        Methods\n",
    "        ----------\n",
    "\n",
    "        - eval_scores: used to evaluate the driver state of attention\n",
    "        - get_PERCLOS: specifically used to evaluate the driver sleepiness\n",
    "        \"\"\"\n",
    "\n",
    "        self.fps = capture_fps\n",
    "        self.delta_time_frame = (1.0 / capture_fps)  # estimated frame time\n",
    "        self.prev_time = 0  # auxiliary variable for the PERCLOS estimation function\n",
    "        self.perclos_time_period = 60  # default time period for PERCLOS (60 seconds)\n",
    "        self.perclos_tresh = perclos_tresh\n",
    "\n",
    "        # the time tresholds are divided for the estimated frame time\n",
    "        # (that is a function passed parameter and so can vary)\n",
    "        self.ear_tresh = ear_tresh\n",
    "        self.ear_act_tresh = ear_time_tresh / self.delta_time_frame\n",
    "        self.ear_counter = 0\n",
    "        self.eye_closure_counter = 0\n",
    "\n",
    "        self.gaze_tresh = gaze_tresh\n",
    "        self.gaze_act_tresh = gaze_time_tresh / self.delta_time_frame\n",
    "        self.gaze_counter = 0\n",
    "\n",
    "        self.roll_tresh = roll_tresh\n",
    "        self.pitch_tresh = pitch_tresh\n",
    "        self.yaw_tresh = yaw_tresh\n",
    "        self.pose_act_tresh = pose_time_tresh / self.delta_time_frame\n",
    "        self.pose_counter = 0\n",
    "\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def eval_scores(self, ear_score, gaze_score, head_roll, head_pitch, head_yaw):\n",
    "\n",
    "        \"\"\"\n",
    "        :param ear_score: float\n",
    "            EAR (Eye Aspect Ratio) score obtained from the driver eye aperture\n",
    "        :param gaze_score: float\n",
    "            Gaze Score obtained from the driver eye gaze\n",
    "        :param head_roll: float\n",
    "            Roll angle obtained from the driver head pose\n",
    "        :param head_pitch: float\n",
    "            Pitch angle obtained from the driver head pose\n",
    "        :param head_yaw: float\n",
    "            Yaw angle obtained from the driver head pose\n",
    "\n",
    "        :return:\n",
    "            Returns a tuple of boolean values that indicates the driver state of attention\n",
    "            tuple: (asleep, looking_away, distracted)\n",
    "        \"\"\"\n",
    "        # instantiating state of attention variables\n",
    "        asleep = False\n",
    "        looking_away = False\n",
    "        distracted = False\n",
    "\n",
    "        if self.ear_counter >= self.ear_act_tresh:  # check if the ear cumulative counter surpassed the threshold\n",
    "            asleep = True\n",
    "\n",
    "        if self.gaze_counter >= self.gaze_act_tresh:  # check if the gaze cumulative counter surpassed the threshold\n",
    "            looking_away = True\n",
    "\n",
    "        if self.pose_counter >= self.pose_act_tresh:  # check if the pose cumulative counter surpassed the threshold\n",
    "            distracted = True\n",
    "\n",
    "        '''\n",
    "        The 3 if blocks that follow are written in a way that when we have a score that's over it's value threshold, \n",
    "        a respective score counter (ear counter, gaze counter, pose counter) is increased and can reach a given maximum \n",
    "        over time.\n",
    "        When a score doesn't surpass a threshold, it is diminished and can go to a minimum of zero.\n",
    "        \n",
    "        Example:\n",
    "        \n",
    "        If the ear score of the eye of the driver surpasses the threshold for a SINGLE frame, the ear_counter is increased.\n",
    "        If the ear score of the eye is surpassed for multiple frames, the ear_counter will be increased and will reach \n",
    "        a given maximum, then it won't increase but the \"asleep\" variable will be set to True.\n",
    "        When the ear_score doesn't surpass the threshold, the ear_counter is decreased. If there are multiple frame\n",
    "        where the score doesn't surpass the threshold, the ear_counter can reach the minimum of zero\n",
    "        \n",
    "        This way, we have a cumulative score for each of the controlled features (EAR, GAZE and HEAD POSE).\n",
    "        If high score it's reached for a cumulative counter, this function will retain its value and will need a\n",
    "        bit of \"cool-down time\" to reach zero again \n",
    "        '''\n",
    "        if (ear_score is not None) and (ear_score <= self.ear_tresh):\n",
    "            if not asleep:\n",
    "                self.ear_counter += 1\n",
    "        elif self.ear_counter > 0:\n",
    "            self.ear_counter -= 1\n",
    "\n",
    "        if (gaze_score is not None) and (gaze_score >= self.gaze_tresh):\n",
    "            if not looking_away:\n",
    "                self.gaze_counter += 1\n",
    "        elif self.gaze_counter > 0:\n",
    "            self.gaze_counter -= 1\n",
    "\n",
    "        if ((self.roll_tresh is not None and head_roll is not None and head_roll > self.roll_tresh) or (\n",
    "                head_pitch is not None and abs(head_pitch) > self.pitch_tresh) or (\n",
    "                head_yaw is not None and abs(head_yaw) > self.yaw_tresh)):\n",
    "            if not distracted:\n",
    "                self.pose_counter += 1\n",
    "        elif self.pose_counter > 0:\n",
    "            self.pose_counter -= 1\n",
    "\n",
    "        if self.verbose:  # print additional info if verbose is True\n",
    "            print(\n",
    "                f\"ear counter:{self.ear_counter}/{self.ear_act_tresh}\\ngaze counter:{self.gaze_counter}/{self.gaze_act_tresh}\\npose counter:{self.pose_counter}/{self.pose_act_tresh}\")\n",
    "            print(\n",
    "                f\"eye closed:{asleep}\\tlooking away:{looking_away}\\tdistracted:{distracted}\")\n",
    "\n",
    "        return asleep, looking_away, distracted\n",
    "\n",
    "    def get_PERCLOS(self, ear_score):\n",
    "        \"\"\"\n",
    "\n",
    "        :param ear_score: float\n",
    "            EAR (Eye Aspect Ratio) score obtained from the driver eye aperture\n",
    "        :return:\n",
    "            tuple:(tired, perclos_score)\n",
    "\n",
    "            tired:\n",
    "                is a boolean value indicating if the driver is tired or not\n",
    "            perclos_score:\n",
    "                is a float value indicating the PERCLOS score over a minute\n",
    "                after a minute this scores resets itself to zero\n",
    "        \"\"\"\n",
    "\n",
    "        delta = time.time() - self.prev_time  # set delta timer\n",
    "        tired = False  # set default value for the tired state of the driver\n",
    "\n",
    "        # if the ear_score is lower or equal than the threshold, increase the eye_closure_counter\n",
    "        if (ear_score is not None) and (ear_score <= self.ear_tresh):\n",
    "            self.eye_closure_counter += 1\n",
    "\n",
    "        closure_time = (self.eye_closure_counter * self.delta_time_frame)  # compute the cumulative eye closure time\n",
    "        perclos_score = (closure_time) / self.perclos_time_period  # compute the PERCLOS over a given time period\n",
    "\n",
    "        if perclos_score >= self.perclos_tresh:  # if the PERCLOS score is higher than a threshold, tired = True\n",
    "            tired = True\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\n",
    "                f\"Closure Time:{closure_time}/{self.perclos_time_period}\\nPERCLOS: {round(perclos_score, 3)}\")\n",
    "\n",
    "        if delta >= self.perclos_time_period:  # at every end of the given time period, reset the counter and the timer\n",
    "            self.eye_closure_counter = 0\n",
    "            self.prev_time = time.time()\n",
    "\n",
    "        return tired, perclos_score\n",
    "\n"
   ]
  },
  {
   "source": [
    "### Main Script\n",
    "The main python scripts is used to find where is the head of the driver, the dlib 68 keypoints of the biggest face found and the processing for all the information about the driver state (eyes aperture and gaze and head pose)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ctime = 0  # current time (used to compute FPS)\n",
    "ptime = 0  # past time (used to compute FPS)\n",
    "prev_time = 0  # previous time variable, used to set the FPS limit\n",
    "fps_lim = 11  # FPS upper limit value, needed for estimating the time for each frame and increasing performances\n",
    "time_lim = 1. / fps_lim  # time window for each frame taken by the webcam\n",
    "\n",
    "Detector = dlib.get_frontal_face_detector()  # instantiation of the dlib face detector object\n",
    "Predictor = dlib.shape_predictor(\"predictor\\shape_predictor_68_face_landmarks.dat\")  # instantiation of the dlib keypoint detector model\n",
    "'''\n",
    "the keypoint predictor is compiled in C++ and saved as a .dat inside the \"predictor\" folder in the project\n",
    "inside the folder there is also a useful face keypoint image map to understand the position and numnber of the\n",
    "various predicted face keypoints\n",
    "'''\n",
    "\n",
    "Scorer = Attention_Scorer(fps_lim, ear_tresh=0.15, ear_time_tresh=2, gaze_tresh=0.2,\n",
    "                            gaze_time_tresh=2, pitch_tresh=35, yaw_tresh=28, pose_time_tresh=2.5, verbose=False)\n",
    "# instantiation of the attention scorer object, with the various thresholds\n",
    "# NOTE: set verbose to True for additional printed information about the scores\n",
    "\n",
    "clahe = cv2.createCLAHE(clipLimit=2, tileGridSize=(8, 8))\n",
    "\n",
    "cap = cv2.VideoCapture(0)  # capture the input from the default system camera (camera number 0)\n",
    "if not cap.isOpened():  # if the camera can't be opened exit the program\n",
    "    print(\"Cannot open camera\")\n",
    "    exit()\n",
    "\n",
    "while True:  # infinite loop for webcam video capture\n",
    "\n",
    "    delta_time = time.time() - prev_time  # delta time for FPS capping\n",
    "    ret, frame = cap.read()  # read a frame from the webcam\n",
    "\n",
    "    if not ret:  # if a frame can't be read, exit the program\n",
    "        print(\"Can't receive frame from camera/stream end\")\n",
    "        break\n",
    "\n",
    "    if delta_time >= time_lim:  # if the time passed is bigger or equal than the frame time, process the frame\n",
    "        prev_time = time.time()\n",
    "\n",
    "        # compute the actual frame rate per second (FPS) of the webcam video capture stream, and show it\n",
    "        ctime = time.time()\n",
    "        fps = 1.0 / float(ctime - ptime)\n",
    "        ptime = ctime\n",
    "        cv2.putText(frame, \"FPS:\" + str(round(fps, 0)), (10, 400), cv2.FONT_HERSHEY_PLAIN, 2,\n",
    "                    (255, 0, 255), 1)\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # transform the BGR frame in grayscale\n",
    "        gray = cv2.bilateralFilter(gray,5,10,10)  # apply a bilateral filter to lower noise but keep frame details\n",
    "\n",
    "        faces = Detector(gray)  # find the faces using the dlib face detector\n",
    "\n",
    "        if len(faces) > 0:  # process the frame only if at least a face is found\n",
    "\n",
    "            # take only the bounding box of the biggest face\n",
    "            faces = sorted(faces, key=get_face_area, reverse=True)\n",
    "            driver_face = faces[0]\n",
    "\n",
    "            landmarks = Predictor(gray, driver_face)  # predict the 68 facial keypoints position\n",
    "\n",
    "            # instantiate the Eye detector and pose estimator objects\n",
    "            Eye_det = Eye_Detector(gray, landmarks, show_processing=False)\n",
    "            Head_pose = Head_Pose_Estimator(\n",
    "                frame, landmarks, verbose=True)\n",
    "\n",
    "            Eye_det.show_eye_keypoints(frame)  # shows the eye keypoints (can be commented)\n",
    "\n",
    "            ear = Eye_det.get_EAR()  # compute the EAR score of the eyes\n",
    "            tired, perclos_score = Scorer.get_PERCLOS(ear)  # compute the PERCLOS score and state of tiredness\n",
    "            gaze = Eye_det.get_Gaze_Score()  # compute the Gaze Score\n",
    "            frame_det, roll, pitch, yaw = Head_pose.get_pose()  # compute the head pose\n",
    "\n",
    "            if frame_det is not None:  # if the head pose estimation is successful, show the results\n",
    "                frame = frame_det\n",
    "\n",
    "            if ear is not None:  # show the real-time EAR score\n",
    "                cv2.putText(frame, \"EAR:\" + str(round(ear, 3)), (10, 50),\n",
    "                            cv2.FONT_HERSHEY_PLAIN, 2, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "            if gaze is not None:  # show the real-time Gaze Score\n",
    "                cv2.putText(frame, \"Gaze Score:\" + str(round(gaze, 3)), (10, 80),\n",
    "                            cv2.FONT_HERSHEY_PLAIN, 2, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "            # show the real-time PERCLOS score\n",
    "            cv2.putText(frame, \"PERCLOS:\" + str(round(perclos_score, 3)), (10, 110),\n",
    "                        cv2.FONT_HERSHEY_PLAIN, 2, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "            if tired:  # if the driver is tired, show and alert on screen\n",
    "                cv2.putText(frame, \"TIRED!\", (10, 280),\n",
    "                            cv2.FONT_HERSHEY_PLAIN, 1, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "            asleep, looking_away, distracted = Scorer.eval_scores(\n",
    "                ear, gaze, roll, pitch, yaw)  # evaluate the scores for EAR, GAZE and HEAD POSE\n",
    "\n",
    "            # if the state of attention of the driver is not normal, show an alert on screen\n",
    "            if asleep:\n",
    "                cv2.putText(frame, \"ASLEEP!\", (10, 300),\n",
    "                            cv2.FONT_HERSHEY_PLAIN, 1, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "            if looking_away:\n",
    "                cv2.putText(frame, \"LOOKING AWAY!\", (10, 320),\n",
    "                            cv2.FONT_HERSHEY_PLAIN, 1, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "            if distracted:\n",
    "                cv2.putText(frame, \"DISTRACTED!\", (10, 340),\n",
    "                            cv2.FONT_HERSHEY_PLAIN, 1, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow(\"Frame\", frame)  # show the frame on screen\n",
    "\n",
    "    # if the key \"q\" is pressed on the keyboard, the program is terminated\n",
    "    if cv2.waitKey(20) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Unable to open predictor\\shape_predictor_68_face_landmarks.dat",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-49944b91a717>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mDetector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_frontal_face_detector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# instantiation of the dlib face detector object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mPredictor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape_predictor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"predictor\\shape_predictor_68_face_landmarks.dat\"\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# instantiation of the dlib keypoint detector model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     '''\n\u001b[0;32m     11\u001b[0m     \u001b[0mthe\u001b[0m \u001b[0mkeypoint\u001b[0m \u001b[0mpredictor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mcompiled\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msaved\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdat\u001b[0m \u001b[0minside\u001b[0m \u001b[0mthe\u001b[0m \u001b[1;34m\"predictor\"\u001b[0m \u001b[0mfolder\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unable to open predictor\\shape_predictor_68_face_landmarks.dat"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}